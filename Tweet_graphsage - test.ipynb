{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import random\n",
    "from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import time\n",
    "import ast\n",
    "\n",
    "\n",
    "import networkx as nx\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import tensorflow as tf\n",
    "\n",
    "from collections import namedtuple\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph(object):\n",
    "    def __init__(self):\n",
    "        self.G = None\n",
    "        self.look_up_dict = {}\n",
    "        self.look_back_list = []\n",
    "        self.node_size = 0\n",
    "\n",
    "    def encode_node(self):\n",
    "        look_up = self.look_up_dict\n",
    "        look_back = self.look_back_list\n",
    "        for node in self.G.nodes():\n",
    "            look_up[node] = self.node_size\n",
    "            look_back.append(node)\n",
    "            self.node_size += 1\n",
    "            self.G.nodes[node]['status'] = ''\n",
    "\n",
    "    def read_g(self, g):\n",
    "        self.G = g\n",
    "        self.encode_node()\n",
    "\n",
    "    def read_adjlist(self, filename):\n",
    "        \"\"\" Read graph from adjacency file in which the edge must be unweighted\n",
    "            the format of each line: v1 n1 n2 n3 ... nk\n",
    "            :param filename: the filename of input file\n",
    "        \"\"\"\n",
    "        self.G = nx.read_adjlist(filename, create_using=nx.DiGraph())\n",
    "        for i, j in self.G.edges():\n",
    "            self.G[i][j]['weight'] = 1.0\n",
    "        self.encode_node()\n",
    "\n",
    "    def read_edgelist(self, filename, weighted=False, directed=False):\n",
    "        self.G = nx.DiGraph()\n",
    "\n",
    "        if directed:\n",
    "            def read_unweighted(l):\n",
    "                src, dst = l.split()\n",
    "                self.G.add_edge(src, dst)\n",
    "                self.G[src][dst]['weight'] = 1.0\n",
    "\n",
    "            def read_weighted(l):\n",
    "                src, dst, w = l.split()\n",
    "                self.G.add_edge(src, dst)\n",
    "                self.G[src][dst]['weight'] = float(w)\n",
    "        else:\n",
    "            def read_unweighted(l):\n",
    "                src, dst = l.split()\n",
    "                self.G.add_edge(src, dst)\n",
    "                self.G.add_edge(dst, src)\n",
    "                self.G[src][dst]['weight'] = 1.0\n",
    "                self.G[dst][src]['weight'] = 1.0\n",
    "\n",
    "            def read_weighted(l):\n",
    "                src, dst, w = l.split()\n",
    "                self.G.add_edge(src, dst)\n",
    "                self.G.add_edge(dst, src)\n",
    "                self.G[src][dst]['weight'] = float(w)\n",
    "                self.G[dst][src]['weight'] = float(w)\n",
    "        fin = open(filename, 'r')\n",
    "        func = read_unweighted\n",
    "        if weighted:\n",
    "            func = read_weighted\n",
    "        while 1:\n",
    "            l = fin.readline()\n",
    "            if l == '':\n",
    "                break\n",
    "            func(l)\n",
    "        fin.close()\n",
    "        self.encode_node()\n",
    "\n",
    "    def read_node_label(self, filename):\n",
    "        fin = open(filename, 'r')\n",
    "        while 1:\n",
    "            l = fin.readline()\n",
    "            if l == '':\n",
    "                break\n",
    "            vec = l.split()\n",
    "            self.G.nodes[vec[0]]['label'] = vec[1:]\n",
    "        fin.close()\n",
    "\n",
    "    def read_node_features(self, filename):\n",
    "        fin = open(filename, 'r')\n",
    "        for l in fin.readlines():\n",
    "            vec = l.split()\n",
    "            self.G.nodes[vec[0]]['feature'] = np.array(\n",
    "                [float(x) for x in vec[1:]])\n",
    "        fin.close()\n",
    "\n",
    "    def read_node_status(self, filename):\n",
    "        fin = open(filename, 'r')\n",
    "        while 1:\n",
    "            l = fin.readline()\n",
    "            if l == '':\n",
    "                break\n",
    "            vec = l.split()\n",
    "            self.G.nodes[vec[0]]['status'] = vec[1]  # train test valid\n",
    "        fin.close()\n",
    "\n",
    "    def read_edge_label(self, filename):\n",
    "        fin = open(filename, 'r')\n",
    "        while 1:\n",
    "            l = fin.readline()\n",
    "            if l == '':\n",
    "                break\n",
    "            vec = l.split()\n",
    "            self.G[vec[0]][vec[1]]['label'] = vec[2:]\n",
    "        fin.close()\n",
    "\n",
    "    \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import numpy \n",
    "\n",
    "class Classifier(object):\n",
    "\n",
    "    def __init__(self, vectors, clf):\n",
    "        self.embeddings = vectors\n",
    "        self.clf = TopKRanker(clf)\n",
    "        self.binarizer = MultiLabelBinarizer(sparse_output=True)\n",
    "\n",
    "    def train(self, X, Y, Y_all):\n",
    "        self.binarizer.fit(Y_all)\n",
    "        X_train = [self.embeddings[int(x)] for x in X]\n",
    "        Y = self.binarizer.transform(Y)\n",
    "        self.clf.fit(X_train, Y)\n",
    "\n",
    "    def evaluate(self, X, Y):\n",
    "        top_k_list = [len(l) for l in Y]\n",
    "        Y_ = self.predict(X, top_k_list)\n",
    "        Y = self.binarizer.transform(Y)\n",
    "        averages = [\"micro\", \"macro\", \"samples\", \"weighted\"]\n",
    "        results = {}\n",
    "        for average in averages:\n",
    "            results[average] = f1_score(Y, Y_, average=average)\n",
    "        results['accuracy'] = accuracy_score(Y, Y_)\n",
    "        \n",
    "        # print('Results, using embeddings of dimensionality', len(self.embeddings[X[0]]))\n",
    "        # print('-------------------')\n",
    "        #print(results)\n",
    "        return results\n",
    "        # print('-------------------')\n",
    "\n",
    "    def predict(self, X, top_k_list):\n",
    "        X_ = numpy.asarray([self.embeddings[int(x)] for x in X])\n",
    "        Y = self.clf.predict(X_, top_k_list=top_k_list)\n",
    "        return Y\n",
    "\n",
    "    def split_train_evaluate(self, X, Y, train_precent, seed=0):\n",
    "        state = numpy.random.get_state()\n",
    "\n",
    "        training_size = int(train_precent * len(X))\n",
    "        numpy.random.seed(seed)\n",
    "        shuffle_indices = numpy.random.permutation(numpy.arange(len(X)))\n",
    "        X_train = [X[shuffle_indices[i]] for i in range(training_size)]\n",
    "        Y_train = [Y[shuffle_indices[i]] for i in range(training_size)]\n",
    "        X_test = [X[shuffle_indices[i]] for i in range(training_size, len(X))]\n",
    "        Y_test = [Y[shuffle_indices[i]] for i in range(training_size, len(X))]\n",
    "\n",
    "        self.train(X_train, Y_train, Y)\n",
    "        numpy.random.set_state(state)\n",
    "        return self.evaluate(X_test, Y_test)\n",
    "    \n",
    "\n",
    "label_file = './../../../data/cora/cora_labels.txt'\n",
    "\n",
    "\n",
    "#label_file = 'group_clean.txt'\n",
    "def read_node_label(filename):\n",
    "    fin = open(filename, 'r')\n",
    "    X = []\n",
    "    Y = []\n",
    "    while 1:\n",
    "        l = fin.readline()\n",
    "        if l == '':\n",
    "            break\n",
    "        vec = l.strip().split(' ')\n",
    "        X.append(vec[0])\n",
    "        Y.append(vec[1:])\n",
    "    fin.close()\n",
    "    return X, Y\n",
    "#vectors = model.vectors\n",
    "#vectors = embedding\n",
    "#vectors = w2v.vectors\n",
    "#vectors = embeddings_concat_norm\n",
    "#X, Y = read_node_label(label_file)\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "class TopKRanker(OneVsRestClassifier):\n",
    "    def predict(self, X, top_k_list):\n",
    "        probs = numpy.asarray(super(TopKRanker, self).predict_proba(X))\n",
    "        all_labels = []\n",
    "        for i, k in enumerate(top_k_list):\n",
    "            probs_ = probs[i, :]\n",
    "            labels = self.classes_[probs_.argsort()[-k:]].tolist()\n",
    "            probs_[:] = 0\n",
    "            probs_[labels] = 1\n",
    "            all_labels.append(probs_)\n",
    "        return numpy.asarray(all_labels)\n",
    "\n",
    "\n",
    "import sys\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "    \n",
    "def glorot(shape, name=None):\n",
    "    \"\"\"Glorot & Bengio (AISTATS 2010) init.\"\"\"\n",
    "    init_range = np.sqrt(6.0/(shape[0]+shape[1]))\n",
    "    initial = tf.random_uniform(shape, minval=-init_range, maxval=init_range, dtype=tf.float32)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "class Model(object):\n",
    "    def __init__(self, **kwargs):\n",
    "        allowed_kwargs = {'name', 'logging', 'model_size'}\n",
    "        for kwarg in kwargs.keys():\n",
    "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
    "        name = kwargs.get('name')\n",
    "        if not name:\n",
    "            name = self.__class__.__name__.lower()\n",
    "        self.name = name\n",
    "\n",
    "        logging = kwargs.get('logging', False)\n",
    "        self.logging = logging\n",
    "\n",
    "        self.vars = {}\n",
    "        self.placeholders = {}\n",
    "\n",
    "        self.layers = []\n",
    "        self.activations = []\n",
    "\n",
    "        self.inputs = None\n",
    "        self.outputs = None\n",
    "\n",
    "        self.loss = 0\n",
    "        self.accuracy = 0\n",
    "        self.optimizer = None\n",
    "        self.opt_op = None\n",
    "\n",
    "    def _build(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def build(self):\n",
    "        \"\"\" Wrapper for _build() \"\"\"\n",
    "        with tf.variable_scope(self.name):\n",
    "            self._build()\n",
    "\n",
    "        # Build sequential layer model\n",
    "        self.activations.append(self.inputs)\n",
    "        for layer in self.layers:\n",
    "            hidden = layer(self.activations[-1])\n",
    "            self.activations.append(hidden)\n",
    "        self.outputs = self.activations[-1]\n",
    "\n",
    "        # Store model variables for easy access\n",
    "        variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name)\n",
    "        self.vars = {var.name: var for var in variables}\n",
    "\n",
    "        # Build metrics\n",
    "        self._loss()\n",
    "        self._accuracy()\n",
    "\n",
    "        self.opt_op = self.optimizer.minimize(self.loss)\n",
    "\n",
    "    def predict(self):\n",
    "        pass\n",
    "\n",
    "    def _loss(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _accuracy(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def save(self, sess=None):\n",
    "        if not sess:\n",
    "            raise AttributeError(\"TensorFlow session not provided.\")\n",
    "        saver = tf.train.Saver(self.vars)\n",
    "        save_path = saver.save(sess, \"tmp/%s.ckpt\" % self.name)\n",
    "        print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "    def load(self, sess=None):\n",
    "        if not sess:\n",
    "            raise AttributeError(\"TensorFlow session not provided.\")\n",
    "        saver = tf.train.Saver(self.vars)\n",
    "        save_path = \"tmp/%s.ckpt\" % self.name\n",
    "        saver.restore(sess, save_path)\n",
    "        print(\"Model restored from file: %s\" % save_path)\n",
    "\n",
    "\n",
    "class MLP(Model):\n",
    "    \"\"\" A standard multi-layer perceptron \"\"\"\n",
    "    def __init__(self, placeholders, dims, categorical=True, **kwargs):\n",
    "        super(MLP, self).__init__(**kwargs)\n",
    "\n",
    "        self.dims = dims\n",
    "        self.input_dim = dims[0]\n",
    "        self.output_dim = dims[-1]\n",
    "        self.placeholders = placeholders\n",
    "        self.categorical = categorical\n",
    "\n",
    "        self.inputs = placeholders['features']\n",
    "        self.labels = placeholders['labels']\n",
    "\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=FLAGS.learning_rate)\n",
    "\n",
    "        self.build()\n",
    "\n",
    "    def _loss(self):\n",
    "        # Weight decay loss\n",
    "        for var in self.layers[0].vars.values():\n",
    "            self.loss += weight_decay * tf.nn.l2_loss(var)\n",
    "\n",
    "        # Cross entropy error\n",
    "        if self.categorical:\n",
    "            self.loss += metrics.masked_softmax_cross_entropy(self.outputs, self.placeholders['labels'],\n",
    "                    self.placeholders['labels_mask'])\n",
    "        # L2\n",
    "        else:\n",
    "            diff = self.labels - self.outputs\n",
    "            self.loss += tf.reduce_sum(tf.sqrt(tf.reduce_sum(diff * diff, axis=1)))\n",
    "\n",
    "    def _accuracy(self):\n",
    "        if self.categorical:\n",
    "            self.accuracy = metrics.masked_accuracy(self.outputs, self.placeholders['labels'],\n",
    "                    self.placeholders['labels_mask'])\n",
    "\n",
    "    def _build(self):\n",
    "        self.layers.append(layers.Dense(input_dim=self.input_dim,\n",
    "                                 output_dim=self.dims[1],\n",
    "                                 act=tf.nn.relu,\n",
    "                                 dropout=self.placeholders['dropout'],\n",
    "                                 sparse_inputs=False,\n",
    "                                 logging=self.logging))\n",
    "\n",
    "        self.layers.append(layers.Dense(input_dim=self.dims[1],\n",
    "                                 output_dim=self.output_dim,\n",
    "                                 act=lambda x: x,\n",
    "                                 dropout=self.placeholders['dropout'],\n",
    "                                 logging=self.logging))\n",
    "\n",
    "    def predict(self):\n",
    "        return tf.nn.softmax(self.outputs)\n",
    "\n",
    "class GeneralizedModel(Model):\n",
    "    \"\"\"\n",
    "    Base class for models that aren't constructed from traditional, sequential layers.\n",
    "    Subclasses must set self.outputs in _build method\n",
    "    (Removes the layers idiom from build method of the Model class)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(GeneralizedModel, self).__init__(**kwargs)\n",
    "        \n",
    "\n",
    "    def build(self):\n",
    "        \"\"\" Wrapper for _build() \"\"\"\n",
    "        with tf.variable_scope(self.name, reuse=True):\n",
    "            self._build()\n",
    "\n",
    "        # Store model variables for easy access\n",
    "        variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name)\n",
    "        self.vars = {var.name: var for var in variables}\n",
    "\n",
    "        # Build metrics\n",
    "        self._loss()\n",
    "        self._accuracy()\n",
    "\n",
    "        self.opt_op = self.optimizer.minimize(self.loss)\n",
    "\n",
    "# SAGEInfo is a namedtuple that specifies the parameters \n",
    "# of the recursive GraphSAGE layers\n",
    "SAGEInfo = namedtuple(\"SAGEInfo\",\n",
    "    ['layer_name', # name of the layer (to get feature embedding etc.)\n",
    "     'neigh_sampler', # callable neigh_sampler constructor\n",
    "     'num_samples',\n",
    "     'output_dim' # the output (i.e., hidden) dimension\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "class UnigramTable:\n",
    "    \"\"\"\n",
    "    Using weight list to initialize the drawing \n",
    "    \"\"\"\n",
    "    def __init__(self, vocab, power=0.75):\n",
    "        vocab_size = len(vocab)\n",
    "        norm = sum([math.pow(t, power) for t in vocab]) # Normalizing constant\n",
    "\n",
    "        table_size = int(1e8) # Length of the unigram table\n",
    "        table = np.zeros(table_size, dtype=np.uint32)\n",
    "\n",
    "        print('Filling unigram table')\n",
    "        p = 0 # Cumulative probability\n",
    "        i = 0\n",
    "        for t in range(vocab_size):\n",
    "            p += float(math.pow(vocab[t], power))/norm\n",
    "            while i < table_size and float(i) / table_size < p:\n",
    "                table[i] = t\n",
    "                i += 1\n",
    "        self.table = table\n",
    "        print('Finish filling unigram table')\n",
    "\n",
    "    def sample(self, count):\n",
    "        indices = np.random.randint(low=0, high=len(self.table), size=count)\n",
    "        return [self.table[i] for i in indices]\n",
    "  \n",
    "\n",
    "        \n",
    "_LAYER_UIDS = {}\n",
    "\n",
    "def get_layer_uid(layer_name=''):\n",
    "    \"\"\"Helper function, assigns unique layer IDs.\"\"\"\n",
    "    if layer_name not in _LAYER_UIDS:\n",
    "        _LAYER_UIDS[layer_name] = 1\n",
    "        return 1\n",
    "    else:\n",
    "        _LAYER_UIDS[layer_name] += 1\n",
    "        return _LAYER_UIDS[layer_name]\n",
    "\n",
    "class Layer(object):\n",
    "    \"\"\"Base layer class. Defines basic API for all layer objects.\n",
    "    Implementation inspired by keras (http://keras.io).\n",
    "    # Properties\n",
    "        name: String, defines the variable scope of the layer.\n",
    "        logging: Boolean, switches Tensorflow histogram logging on/off\n",
    "    # Methods\n",
    "        _call(inputs): Defines computation graph of layer\n",
    "            (i.e. takes input, returns output)\n",
    "        __call__(inputs): Wrapper for _call()\n",
    "        _log_vars(): Log all variables\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        allowed_kwargs = {'name', 'logging', 'model_size'}\n",
    "        for kwarg in kwargs.keys():\n",
    "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
    "        name = kwargs.get('name')\n",
    "        if not name:\n",
    "            layer = self.__class__.__name__.lower()\n",
    "            name = layer + '_' + str(get_layer_uid(layer))\n",
    "        self.name = name\n",
    "        self.vars = {}\n",
    "        logging = kwargs.get('logging', False)\n",
    "        self.logging = logging\n",
    "        self.sparse_inputs = False\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        return inputs\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        with tf.name_scope(self.name):\n",
    "            if self.logging and not self.sparse_inputs:\n",
    "                tf.summary.histogram(self.name + '/inputs', inputs)\n",
    "            outputs = self._call(inputs)\n",
    "            if self.logging:\n",
    "                tf.summary.histogram(self.name + '/outputs', outputs)\n",
    "            return outputs\n",
    "\n",
    "    def _log_vars(self):\n",
    "        for var in self.vars:\n",
    "            tf.summary.histogram(self.name + '/vars/' + var, self.vars[var])\n",
    "            \n",
    "def zeros(shape, name=None):\n",
    "    \"\"\"All zeros.\"\"\"\n",
    "    initial = tf.zeros(shape, dtype=tf.float32)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "class Dense(Layer):\n",
    "    \"\"\"Dense layer.\"\"\"\n",
    "    def __init__(self, input_dim, output_dim, dropout=0., \n",
    "                 act=tf.nn.relu, placeholders=None, bias=True, featureless=False, \n",
    "                 sparse_inputs=False, **kwargs):\n",
    "        super(Dense, self).__init__(**kwargs)\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.act = act\n",
    "        self.featureless = featureless\n",
    "        self.bias = bias\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # helper variable for sparse dropout\n",
    "        self.sparse_inputs = sparse_inputs\n",
    "        if sparse_inputs:\n",
    "            self.num_features_nonzero = placeholders['num_features_nonzero']\n",
    "\n",
    "        with tf.variable_scope(self.name + '_vars'):\n",
    "            self.vars['weights'] = tf.get_variable('weights', shape=(input_dim, output_dim),\n",
    "                                         dtype=tf.float32, \n",
    "                                         initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                         regularizer=tf.contrib.layers.l2_regularizer(weight_decay))\n",
    "            if self.bias:\n",
    "                self.vars['bias'] = zeros([output_dim], name='bias')\n",
    "\n",
    "        if self.logging:\n",
    "            self._log_vars()\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        x = inputs\n",
    "\n",
    "        x = tf.nn.dropout(x, 1-self.dropout)\n",
    "\n",
    "        # transform\n",
    "        output = tf.matmul(x, self.vars['weights'])\n",
    "\n",
    "        # bias\n",
    "        if self.bias:\n",
    "            output += self.vars['bias']\n",
    "\n",
    "        return self.act(output)\n",
    "    \n",
    "class BipartiteEdgePredLayer(Layer):\n",
    "    def __init__(self, input_dim1, input_dim2, placeholders, dropout=False, act=tf.nn.sigmoid,\n",
    "            loss_fn='xent', neg_sample_weights=1.0,\n",
    "            bias=False, bilinear_weights=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Basic class that applies skip-gram-like loss\n",
    "        (i.e., dot product of node+target and node and negative samples)\n",
    "        Args:\n",
    "            bilinear_weights: use a bilinear weight for affinity calculation: u^T A v. If set to\n",
    "                false, it is assumed that input dimensions are the same and the affinity will be \n",
    "                based on dot product.\n",
    "        \"\"\"\n",
    "        super(BipartiteEdgePredLayer, self).__init__(**kwargs)\n",
    "        self.input_dim1 = input_dim1\n",
    "        self.input_dim2 = input_dim2\n",
    "        self.act = act\n",
    "        self.bias = bias\n",
    "        self.eps = 1e-7\n",
    "\n",
    "        # Margin for hinge loss\n",
    "        self.margin = 0.1\n",
    "        self.neg_sample_weights = neg_sample_weights\n",
    "\n",
    "        self.bilinear_weights = bilinear_weights\n",
    "\n",
    "        if dropout:\n",
    "            self.dropout = placeholders['dropout']\n",
    "        else:\n",
    "            self.dropout = 0.\n",
    "\n",
    "        self.neg_sample_size = placeholders[\"neg_sample_size\"]\n",
    "        self.batch_size = placeholders[\"batch_size\"]\n",
    "        \n",
    "        print(\"self.neg_sample_size \",self.neg_sample_size)\n",
    "        print(\"self.batch_size \",self.batch_size)\n",
    "        \n",
    "        # output a likelihood term\n",
    "        self.output_dim = 1\n",
    "        with tf.variable_scope(self.name + '_vars'):\n",
    "            # bilinear form\n",
    "            if bilinear_weights:\n",
    "                #self.vars['weights'] = glorot([input_dim1, input_dim2],\n",
    "                #                              name='pred_weights')\n",
    "                self.vars['weights'] = tf.get_variable(\n",
    "                        'pred_weights', \n",
    "                        shape=(input_dim1, input_dim2),\n",
    "                        dtype=tf.float32, \n",
    "                        initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "            if self.bias:\n",
    "                self.vars['bias'] = zeros([self.output_dim], name='bias')\n",
    "\n",
    "        if loss_fn == 'xent':\n",
    "            self.loss_fn = self._xent_loss\n",
    "        elif loss_fn == 'skipgram':\n",
    "            self.loss_fn = self._skipgram_loss\n",
    "        elif loss_fn == 'hinge':\n",
    "            self.loss_fn = self._hinge_loss\n",
    "\n",
    "        print(\"loss_fn \",loss_fn)\n",
    "        \n",
    "        if self.logging:\n",
    "            self._log_vars()\n",
    "\n",
    "    def affinity(self, inputs1, inputs2):\n",
    "        \"\"\" Affinity score between batch of inputs1 and inputs2.\n",
    "        Args:\n",
    "            inputs1: tensor of shape [batch_size x feature_size].\n",
    "        \"\"\"\n",
    "        # shape: [batch_size, input_dim1]\n",
    "        if self.bilinear_weights:\n",
    "            prod = tf.matmul(inputs2, tf.transpose(self.vars['weights']))\n",
    "            self.prod = prod\n",
    "            result = tf.reduce_sum(inputs1 * prod, axis=1)\n",
    "        else:\n",
    "            result = tf.reduce_sum(inputs1 * inputs2, axis=1)\n",
    "        return result\n",
    "\n",
    "    def neg_cost(self, inputs1, neg_samples, hard_neg_samples=None):\n",
    "        \"\"\" For each input in batch, compute the sum of its affinity to negative samples.\n",
    "        Returns:\n",
    "            Tensor of shape [batch_size x num_neg_samples]. For each node, a list of affinities to\n",
    "                negative samples is computed.\n",
    "        \"\"\"\n",
    "        if self.bilinear_weights:\n",
    "            inputs1 = tf.matmul(inputs1, self.vars['weights'])\n",
    "        #neg_aff = tf.matmul(inputs1, tf.transpose(neg_samples))\n",
    "        #neg_aff =  tf.reduce_sum(inputs1 * neg_samples, axis=1)\n",
    "        \n",
    "        \n",
    "        shape = inputs1.shape\n",
    "        dim = shape[1]\n",
    "        \n",
    "        print(\"orginal neg_samples shape \",neg_samples.shape)\n",
    "        \n",
    "        neg_samples = tf.reshape(neg_samples, [self.batch_size, self.neg_sample_size, dim])\n",
    "        \n",
    "        print(\"reshape1 neg_samples shape \",neg_samples.shape)\n",
    "        \n",
    "        neg_samples = tf.transpose(neg_samples, perm=[0, 2, 1])\n",
    "        \n",
    "        print(\"reshape2 neg_samples shape \",neg_samples.shape)\n",
    "        \n",
    "        print(\"orginal inputs1 shape \",inputs1.shape)\n",
    "        \n",
    "        inputs1 = tf.reshape(inputs1, [self.batch_size, 1, dim])\n",
    "        \n",
    "        temp_neg_aff = tf.matmul(inputs1, neg_samples)\n",
    "        print(\"temp_neg_aff shape \",temp_neg_aff.shape)\n",
    "        \n",
    "        neg_aff = tf.squeeze(tf.matmul(inputs1, neg_samples),[1])\n",
    "        \n",
    "        print(\"neg_samples shape \",neg_samples.shape)\n",
    "        print(\"inputs1 shape \",inputs1.shape)\n",
    "        print(\"neg_aff shape \",neg_aff.shape)\n",
    "        \n",
    "        return neg_aff\n",
    "\n",
    "    def loss(self, inputs1, inputs2, neg_samples):\n",
    "        \"\"\" negative sampling loss.\n",
    "        Args:\n",
    "            neg_samples: tensor of shape [num_neg_samples x input_dim2]. Negative samples for all\n",
    "            inputs in batch inputs1.\n",
    "        \"\"\"\n",
    "        return self.loss_fn(inputs1, inputs2, neg_samples)\n",
    "\n",
    "    def _xent_loss(self, inputs1, inputs2, neg_samples, hard_neg_samples=None):\n",
    "        aff = self.affinity(inputs1, inputs2)\n",
    "        \n",
    "        neg_aff = self.neg_cost(inputs1, neg_samples, hard_neg_samples)\n",
    "        \n",
    "        true_xent = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                labels=tf.ones_like(aff), logits=aff)\n",
    "        negative_xent = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                labels=tf.zeros_like(neg_aff), logits=neg_aff)\n",
    "        \n",
    "        loss = tf.reduce_sum(true_xent) + self.neg_sample_weights * tf.reduce_sum(negative_xent)\n",
    "        return loss\n",
    "\n",
    "    def _skipgram_loss(self, inputs1, inputs2, neg_samples, hard_neg_samples=None):\n",
    "        aff = self.affinity(inputs1, inputs2)\n",
    "        neg_aff = self.neg_cost(inputs1, neg_samples, hard_neg_samples)\n",
    "        neg_cost = tf.log(tf.reduce_sum(tf.exp(neg_aff), axis=1))\n",
    "        loss = tf.reduce_sum(aff - neg_cost)\n",
    "        return loss\n",
    "\n",
    "    def _hinge_loss(self, inputs1, inputs2, neg_samples, hard_neg_samples=None):\n",
    "        aff = self.affinity(inputs1, inputs2)\n",
    "        neg_aff = self.neg_cost(inputs1, neg_samples, hard_neg_samples)\n",
    "        diff = tf.nn.relu(tf.subtract(neg_aff, tf.expand_dims(aff, 1) - self.margin), name='diff')\n",
    "        loss = tf.reduce_sum(diff)\n",
    "        self.neg_shape = tf.shape(neg_aff)\n",
    "        return loss\n",
    "\n",
    "    def weights_norm(self):\n",
    "        return tf.nn.l2_norm(self.vars['weights'])\n",
    "    \n",
    "    \n",
    "class UniformNeighborSampler(Layer):\n",
    "    \"\"\"\n",
    "    Uniformly samples neighbors.\n",
    "    Assumes that adj lists are padded with random re-sampling\n",
    "    \"\"\"\n",
    "    def __init__(self, adj_info, **kwargs):\n",
    "        super(UniformNeighborSampler, self).__init__(**kwargs)\n",
    "        self.adj_info = adj_info\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        ids, num_samples = inputs\n",
    "        adj_lists = tf.nn.embedding_lookup(self.adj_info, ids) \n",
    "        adj_lists = tf.transpose(tf.random_shuffle(tf.transpose(adj_lists)))\n",
    "        adj_lists = tf.slice(adj_lists, [0,0], [-1, num_samples])\n",
    "        return adj_lists\n",
    "\n",
    "    \n",
    "class GCNAggregator(Layer):\n",
    "    \"\"\"\n",
    "    Aggregates via mean followed by matmul and non-linearity.\n",
    "    Same matmul parameters are used self vector and neighbor vectors.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, neigh_input_dim=None,\n",
    "            dropout=0., bias=False, act=tf.nn.relu, name=None, concat=False, **kwargs):\n",
    "        super(GCNAggregator, self).__init__(**kwargs)\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.bias = bias\n",
    "        self.act = act\n",
    "        self.concat = concat\n",
    "\n",
    "        if neigh_input_dim is None:\n",
    "            neigh_input_dim = input_dim\n",
    "\n",
    "        if name is not None:\n",
    "            name = '/' + name\n",
    "        else:\n",
    "            name = ''\n",
    "\n",
    "        with tf.variable_scope(self.name + name + '_vars'):\n",
    "            #scope.reuse_variables()  \n",
    "            \n",
    "            self.vars['weights'] = glorot([neigh_input_dim, output_dim],\n",
    "                                                        name='neigh_weights')\n",
    "            if self.bias:\n",
    "                self.vars['bias'] = zeros([self.output_dim], name='bias')\n",
    "\n",
    "        if self.logging:\n",
    "            self._log_vars()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        self_vecs, neigh_vecs = inputs\n",
    "\n",
    "        neigh_vecs = tf.nn.dropout(neigh_vecs, 1-self.dropout)\n",
    "        self_vecs = tf.nn.dropout(self_vecs, 1-self.dropout)\n",
    "        means = tf.reduce_mean(tf.concat([neigh_vecs, \n",
    "            tf.expand_dims(self_vecs, axis=1)], axis=1), axis=1)\n",
    "       \n",
    "        # [nodes] x [out_dim]\n",
    "        output = tf.matmul(means, self.vars['weights'])\n",
    "\n",
    "        # bias\n",
    "        if self.bias:\n",
    "            output += self.vars['bias']\n",
    "       \n",
    "        return self.act(output)    \n",
    "class MeanPoolingAggregator(Layer):\n",
    "    \"\"\" Aggregates via mean-pooling over MLP functions.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim, model_size=\"small\", neigh_input_dim=None,\n",
    "            dropout=0., bias=False, act=tf.nn.relu, name=None, concat=False, **kwargs):\n",
    "        super(MeanPoolingAggregator, self).__init__(**kwargs)\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.bias = bias\n",
    "        self.act = act\n",
    "        self.concat = concat\n",
    "\n",
    "        if neigh_input_dim is None:\n",
    "            neigh_input_dim = input_dim\n",
    "\n",
    "        if name is not None:\n",
    "            name = '/' + name\n",
    "        else:\n",
    "            name = ''\n",
    "\n",
    "        if model_size == \"small\":\n",
    "            hidden_dim = self.hidden_dim = 512\n",
    "        elif model_size == \"big\":\n",
    "            hidden_dim = self.hidden_dim = 1024\n",
    "\n",
    "        self.mlp_layers = []\n",
    "        self.mlp_layers.append(Dense(input_dim=neigh_input_dim,\n",
    "                                 output_dim=hidden_dim,\n",
    "                                 act=tf.nn.relu,\n",
    "                                 dropout=dropout,\n",
    "                                 sparse_inputs=False,\n",
    "                                 logging=self.logging))\n",
    "\n",
    "        with tf.variable_scope(self.name + name + '_vars'):\n",
    "            self.vars['neigh_weights'] = glorot([hidden_dim, output_dim],\n",
    "                                                        name='neigh_weights')\n",
    "           \n",
    "            self.vars['self_weights'] = glorot([input_dim, output_dim],\n",
    "                                                        name='self_weights')\n",
    "            if self.bias:\n",
    "                self.vars['bias'] = zeros([self.output_dim], name='bias')\n",
    "\n",
    "        if self.logging:\n",
    "            self._log_vars()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.neigh_input_dim = neigh_input_dim\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        self_vecs, neigh_vecs = inputs\n",
    "        neigh_h = neigh_vecs\n",
    "\n",
    "        dims = tf.shape(neigh_h)\n",
    "        batch_size = dims[0]\n",
    "        num_neighbors = dims[1]\n",
    "        # [nodes * sampled neighbors] x [hidden_dim]\n",
    "        h_reshaped = tf.reshape(neigh_h, (batch_size * num_neighbors, self.neigh_input_dim))\n",
    "\n",
    "        for l in self.mlp_layers:\n",
    "            h_reshaped = l(h_reshaped)\n",
    "        neigh_h = tf.reshape(h_reshaped, (batch_size, num_neighbors, self.hidden_dim))\n",
    "        neigh_h = tf.reduce_mean(neigh_h, axis=1)\n",
    "        \n",
    "        from_neighs = tf.matmul(neigh_h, self.vars['neigh_weights'])\n",
    "        from_self = tf.matmul(self_vecs, self.vars[\"self_weights\"])\n",
    "        \n",
    "        if not self.concat:\n",
    "            output = tf.add_n([from_self, from_neighs])\n",
    "        else:\n",
    "            output = tf.concat([from_self, from_neighs], axis=1)\n",
    "\n",
    "        # bias\n",
    "        if self.bias:\n",
    "            output += self.vars['bias']\n",
    "       \n",
    "        return self.act(output)\n",
    "    \n",
    "class MeanAggregator(Layer):\n",
    "    \"\"\"\n",
    "    Aggregates via mean followed by matmul and non-linearity.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, neigh_input_dim=None,\n",
    "            dropout=0., bias=False, act=tf.nn.relu, \n",
    "            name=None, concat=False, **kwargs):\n",
    "        super(MeanAggregator, self).__init__(**kwargs)\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.bias = bias\n",
    "        self.act = act\n",
    "        self.concat = concat\n",
    "\n",
    "        if neigh_input_dim is None:\n",
    "            neigh_input_dim = input_dim\n",
    "\n",
    "        if name is not None:\n",
    "            name = '/' + name\n",
    "        else:\n",
    "            name = ''\n",
    "\n",
    "        with tf.variable_scope(self.name + name + '_vars'):\n",
    "            self.vars['neigh_weights'] = glorot([neigh_input_dim, output_dim],\n",
    "                                                        name='neigh_weights')\n",
    "            self.vars['self_weights'] = glorot([input_dim, output_dim],\n",
    "                                                        name='self_weights')\n",
    "            if self.bias:\n",
    "                self.vars['bias'] = zeros([self.output_dim], name='bias')\n",
    "\n",
    "        if self.logging:\n",
    "            self._log_vars()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        self_vecs, neigh_vecs = inputs\n",
    "\n",
    "        neigh_vecs = tf.nn.dropout(neigh_vecs, 1-self.dropout)\n",
    "        self_vecs = tf.nn.dropout(self_vecs, 1-self.dropout)\n",
    "        neigh_means = tf.reduce_mean(neigh_vecs, axis=1)\n",
    "       \n",
    "        # [nodes] x [out_dim]\n",
    "        from_neighs = tf.matmul(neigh_means, self.vars['neigh_weights'])\n",
    "\n",
    "        from_self = tf.matmul(self_vecs, self.vars[\"self_weights\"])\n",
    "         \n",
    "        if not self.concat:\n",
    "            output = tf.add_n([from_self, from_neighs])\n",
    "        else:\n",
    "            output = tf.concat([from_self, from_neighs], axis=1)\n",
    "\n",
    "        # bias\n",
    "        if self.bias:\n",
    "            output += self.vars['bias']\n",
    "       \n",
    "        return self.act(output)\n",
    "\n",
    "class SeqAggregator(Layer):\n",
    "    \"\"\" Aggregates via a standard LSTM.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim, model_size=\"small\", neigh_input_dim=None,\n",
    "            dropout=0., bias=False, act=tf.nn.relu, name=None,  concat=False, **kwargs):\n",
    "        super(SeqAggregator, self).__init__(**kwargs)\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.bias = bias\n",
    "        self.act = act\n",
    "        self.concat = concat\n",
    "\n",
    "        if neigh_input_dim is None:\n",
    "            neigh_input_dim = input_dim\n",
    "\n",
    "        if name is not None:\n",
    "            name = '/' + name\n",
    "        else:\n",
    "            name = ''\n",
    "\n",
    "        if model_size == \"small\":\n",
    "            hidden_dim = self.hidden_dim = 128\n",
    "        elif model_size == \"big\":\n",
    "            hidden_dim = self.hidden_dim = 256\n",
    "\n",
    "        with tf.variable_scope(self.name + name + '_vars'):\n",
    "            self.vars['neigh_weights'] = glorot([hidden_dim, output_dim],\n",
    "                                                        name='neigh_weights')\n",
    "           \n",
    "            self.vars['self_weights'] = glorot([input_dim, output_dim],\n",
    "                                                        name='self_weights')\n",
    "            if self.bias:\n",
    "                self.vars['bias'] = zeros([self.output_dim], name='bias')\n",
    "\n",
    "        if self.logging:\n",
    "            self._log_vars()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.neigh_input_dim = neigh_input_dim\n",
    "        self.cell = tf.contrib.rnn.BasicLSTMCell(self.hidden_dim)\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        self_vecs, neigh_vecs = inputs\n",
    "\n",
    "        dims = tf.shape(neigh_vecs)\n",
    "        batch_size = dims[0]\n",
    "        initial_state = self.cell.zero_state(batch_size, tf.float32)\n",
    "        used = tf.sign(tf.reduce_max(tf.abs(neigh_vecs), axis=2))\n",
    "        length = tf.reduce_sum(used, axis=1)\n",
    "        length = tf.maximum(length, tf.constant(1.))\n",
    "        length = tf.cast(length, tf.int32)\n",
    "\n",
    "        with tf.variable_scope(self.name) as scope:\n",
    "            try:\n",
    "                rnn_outputs, rnn_states = tf.nn.dynamic_rnn(\n",
    "                        self.cell, neigh_vecs,\n",
    "                        initial_state=initial_state, dtype=tf.float32, time_major=False,\n",
    "                        sequence_length=length)\n",
    "            except ValueError:\n",
    "                scope.reuse_variables()\n",
    "                rnn_outputs, rnn_states = tf.nn.dynamic_rnn(\n",
    "                        self.cell, neigh_vecs,\n",
    "                        initial_state=initial_state, dtype=tf.float32, time_major=False,\n",
    "                        sequence_length=length)\n",
    "        batch_size = tf.shape(rnn_outputs)[0]\n",
    "        max_len = tf.shape(rnn_outputs)[1]\n",
    "        out_size = int(rnn_outputs.get_shape()[2])\n",
    "        index = tf.range(0, batch_size) * max_len + (length - 1)\n",
    "        flat = tf.reshape(rnn_outputs, [-1, out_size])\n",
    "        neigh_h = tf.gather(flat, index)\n",
    "\n",
    "        from_neighs = tf.matmul(neigh_h, self.vars['neigh_weights'])\n",
    "        from_self = tf.matmul(self_vecs, self.vars[\"self_weights\"])\n",
    "         \n",
    "        output = tf.add_n([from_self, from_neighs])\n",
    "\n",
    "        if not self.concat:\n",
    "            output = tf.add_n([from_self, from_neighs])\n",
    "        else:\n",
    "            output = tf.concat([from_self, from_neighs], axis=1)\n",
    "\n",
    "        # bias\n",
    "        if self.bias:\n",
    "            output += self.vars['bias']\n",
    "       \n",
    "        return self.act(output)\n",
    "    \n",
    "class MaxPoolingAggregator(Layer):\n",
    "    \"\"\" Aggregates via max-pooling over MLP functions.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim, model_size=\"small\", neigh_input_dim=None,\n",
    "            dropout=0., bias=False, act=tf.nn.relu, name=None, concat=False, **kwargs):\n",
    "        super(MaxPoolingAggregator, self).__init__(**kwargs)\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.bias = bias\n",
    "        self.act = act\n",
    "        self.concat = concat\n",
    "\n",
    "        if neigh_input_dim is None:\n",
    "            neigh_input_dim = input_dim\n",
    "\n",
    "        if name is not None:\n",
    "            name = '/' + name\n",
    "        else:\n",
    "            name = ''\n",
    "\n",
    "        if model_size == \"small\":\n",
    "            hidden_dim = self.hidden_dim = 512\n",
    "        elif model_size == \"big\":\n",
    "            hidden_dim = self.hidden_dim = 1024\n",
    "\n",
    "        self.mlp_layers = []\n",
    "        self.mlp_layers.append(Dense(input_dim=neigh_input_dim,\n",
    "                                 output_dim=hidden_dim,\n",
    "                                 act=tf.nn.relu,\n",
    "                                 dropout=dropout,\n",
    "                                 sparse_inputs=False,\n",
    "                                 logging=self.logging))\n",
    "\n",
    "        with tf.variable_scope(self.name + name + '_vars'):\n",
    "            self.vars['neigh_weights'] = glorot([hidden_dim, output_dim],\n",
    "                                                        name='neigh_weights')\n",
    "           \n",
    "            self.vars['self_weights'] = glorot([input_dim, output_dim],\n",
    "                                                        name='self_weights')\n",
    "            if self.bias:\n",
    "                self.vars['bias'] = zeros([self.output_dim], name='bias')\n",
    "\n",
    "        if self.logging:\n",
    "            self._log_vars()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.neigh_input_dim = neigh_input_dim\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        self_vecs, neigh_vecs = inputs\n",
    "        neigh_h = neigh_vecs\n",
    "\n",
    "        dims = tf.shape(neigh_h)\n",
    "        batch_size = dims[0]\n",
    "        num_neighbors = dims[1]\n",
    "        # [nodes * sampled neighbors] x [hidden_dim]\n",
    "        h_reshaped = tf.reshape(neigh_h, (batch_size * num_neighbors, self.neigh_input_dim))\n",
    "\n",
    "        for l in self.mlp_layers:\n",
    "            h_reshaped = l(h_reshaped)\n",
    "        neigh_h = tf.reshape(h_reshaped, (batch_size, num_neighbors, self.hidden_dim))\n",
    "        neigh_h = tf.reduce_max(neigh_h, axis=1)\n",
    "        \n",
    "        from_neighs = tf.matmul(neigh_h, self.vars['neigh_weights'])\n",
    "        from_self = tf.matmul(self_vecs, self.vars[\"self_weights\"])\n",
    "        \n",
    "        if not self.concat:\n",
    "            output = tf.add_n([from_self, from_neighs])\n",
    "        else:\n",
    "            output = tf.concat([from_self, from_neighs], axis=1)\n",
    "\n",
    "        # bias\n",
    "        if self.bias:\n",
    "            output += self.vars['bias']\n",
    "       \n",
    "        return self.act(output)\n",
    "    \n",
    "class SampleAndAggregate(GeneralizedModel):\n",
    "    \"\"\"\n",
    "    Base implementation of unsupervised GraphSAGE\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, placeholders, features, adj, degrees,\n",
    "            layer_infos, concat=True, aggregator_type=\"mean\", \n",
    "            model_size=\"small\", identity_dim=0,\n",
    "            **kwargs):\n",
    "        '''\n",
    "        Args:\n",
    "            - placeholders: Stanford TensorFlow placeholder object.\n",
    "            - features: Numpy array with node features. \n",
    "                        NOTE: Pass a None object to train in featureless mode (identity features for nodes)!\n",
    "            - adj: Numpy array with adjacency lists (padded with random re-samples)\n",
    "            - degrees: Numpy array with node degrees. \n",
    "            - layer_infos: List of SAGEInfo namedtuples that describe the parameters of all \n",
    "                   the recursive layers. See SAGEInfo definition above.\n",
    "            - concat: whether to concatenate during recursive iterations\n",
    "            - aggregator_type: how to aggregate neighbor information\n",
    "            - model_size: one of \"small\" and \"big\"\n",
    "            - identity_dim: Set to positive int to use identity features (slow and cannot generalize, but better accuracy)\n",
    "        '''\n",
    "        super(SampleAndAggregate, self).__init__(**kwargs)\n",
    "        if aggregator_type == \"mean\":\n",
    "            self.aggregator_cls = MeanAggregator\n",
    "        elif aggregator_type == \"seq\":\n",
    "            self.aggregator_cls = SeqAggregator\n",
    "        elif aggregator_type == \"maxpool\":\n",
    "            self.aggregator_cls = MaxPoolingAggregator\n",
    "        elif aggregator_type == \"meanpool\":\n",
    "            self.aggregator_cls = MeanPoolingAggregator\n",
    "        elif aggregator_type == \"gcn\":\n",
    "            self.aggregator_cls = GCNAggregator\n",
    "        else:\n",
    "            raise Exception(\"Unknown aggregator: \", self.aggregator_cls)\n",
    "\n",
    "        # get info from placeholders...\n",
    "        self.inputs1 = placeholders[\"batch1\"]\n",
    "        self.inputs2 = placeholders[\"batch2\"]\n",
    "        self.neg_samples = placeholders[\"neg_samples\"]\n",
    "        self.neg_sample_size = placeholders[\"neg_sample_size\"]\n",
    "        \n",
    "        self.model_size = model_size\n",
    "        self.adj_info = adj\n",
    "        #self.sample_table = sample_table\n",
    "        \n",
    "        if identity_dim > 0:\n",
    "            #print(\"tf.global_variables() \",tf.global_variables())\n",
    "            tmp = [v for v in tf.global_variables() if v.name == \"node_embeddings:0\"]\n",
    "            print(\"tmp \",tmp)\n",
    "            print(\"adj shape \",adj.get_shape().as_list()[0])\n",
    "            if tmp:\n",
    "                self.embeds = tmp[0]\n",
    "            else:\n",
    "                print(\"tmp is None \")\n",
    "                #self.embeds = tf.get_variable(\"node_embeddings\", [adj.get_shape().as_list()[0], identity_dim])\n",
    "                print(\"(adj.get_shape().as_list()[0] \",(adj.get_shape().as_list()[0]))\n",
    "                self.embeds = tf.get_variable(\"node_embeddings\", initializer= tf.eye(adj.get_shape().as_list()[0], num_columns = identity_dim, dtype=tf.float32) )\n",
    "                #self.embeds = tf.get_variable(\"node_embeddings\", initializer= tf.ones((adj.get_shape().as_list()[0], identity_dim), dtype=tf.float32) )\n",
    "                #self.embeds = tf.get_variable(\"node_embeddings\", initializer= tf.eye(adj.get_shape().as_list()[0], dtype=tf.float32) )\n",
    "                #self.embeds = adj.get_shape().as_list()[0]\n",
    "                #tf.Variable(initial_value = np.identity(784))\n",
    "                \n",
    "        else:\n",
    "           self.embeds = None\n",
    "        if features is None: \n",
    "            if identity_dim == 0:\n",
    "                raise Exception(\"Must have a positive value for identity feature dimension if no input features given.\")\n",
    "            self.features = self.embeds\n",
    "            print(\"self.features \",self.features)\n",
    "        else:\n",
    "            self.features = tf.Variable(tf.constant(features, dtype=tf.float32), trainable=False)\n",
    "            if not self.embeds is None:\n",
    "                self.features = tf.concat([self.embeds, self.features], axis=1)\n",
    "        self.degrees = degrees\n",
    "        self.concat = concat\n",
    "\n",
    "        self.dims = [(0 if features is None else features.shape[1]) + identity_dim]\n",
    "        self.dims.extend([layer_infos[i].output_dim for i in range(len(layer_infos))])\n",
    "        #self.neg_sample_size = placeholders[\"neg_sample_size\"]\n",
    "        self.batch_size = placeholders[\"batch_size\"]\n",
    "        self.placeholders = placeholders\n",
    "        self.layer_infos = layer_infos\n",
    "        \n",
    "        #self.neg_sample_size = 20\n",
    "\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=0.00001)\n",
    "\n",
    "        self.build()\n",
    "\n",
    "    def sample(self, inputs, layer_infos, batch_size=None):\n",
    "        \"\"\" Sample neighbors to be the supportive fields for multi-layer convolutions.\n",
    "        Args:\n",
    "            inputs: batch inputs\n",
    "            batch_size: the number of inputs (different for batch inputs and negative samples).\n",
    "        \"\"\"\n",
    "        \n",
    "        if batch_size is None:\n",
    "            batch_size = self.batch_size\n",
    "        samples = [inputs]\n",
    "        # size of convolution support at each layer per node\n",
    "        support_size = 1\n",
    "        support_sizes = [support_size]\n",
    "        for k in range(len(layer_infos)):\n",
    "            t = len(layer_infos) - k - 1\n",
    "            support_size *= layer_infos[t].num_samples\n",
    "            sampler = layer_infos[t].neigh_sampler\n",
    "            node = sampler((samples[k], layer_infos[t].num_samples))\n",
    "            samples.append(tf.reshape(node, [support_size * batch_size,]))\n",
    "            support_sizes.append(support_size)\n",
    "        return samples, support_sizes\n",
    "\n",
    "\n",
    "    def aggregate(self, samples, input_features, dims, num_samples, support_sizes, batch_size=None,\n",
    "            aggregators=None, name=None, concat=False, model_size=\"small\"):\n",
    "        \"\"\" At each layer, aggregate hidden representations of neighbors to compute the hidden representations \n",
    "            at next layer.\n",
    "        Args:\n",
    "            samples: a list of samples of variable hops away for convolving at each layer of the\n",
    "                network. Length is the number of layers + 1. Each is a vector of node indices.\n",
    "            input_features: the input features for each sample of various hops away.\n",
    "            dims: a list of dimensions of the hidden representations from the input layer to the\n",
    "                final layer. Length is the number of layers + 1.\n",
    "            num_samples: list of number of samples for each layer.\n",
    "            support_sizes: the number of nodes to gather information from for each layer.\n",
    "            batch_size: the number of inputs (different for batch inputs and negative samples).\n",
    "        Returns:\n",
    "            The hidden representation at the final layer for all nodes in batch\n",
    "        \"\"\"\n",
    "\n",
    "        if batch_size is None:\n",
    "            batch_size = self.batch_size\n",
    "\n",
    "        # length: number of layers + 1\n",
    "        hidden = [tf.nn.embedding_lookup(input_features, node_samples) for node_samples in samples]\n",
    "        \n",
    "#         tmp_array_dim = input_features[0]\n",
    "#         tmp_array_row = len(samples)\n",
    "#         print(\"samples len: \", len(samples))\n",
    "#         print(\"samples : \", samples)\n",
    "        \n",
    "#         hidden = np.zeros((tmp_array_row,tmp_array_dim))\n",
    "#         for i, node_samples in enumerate(samples):\n",
    "#             hidden[i,node_samples] = 1\n",
    "        #print(hidden)\n",
    "        \n",
    "        \n",
    "        new_agg = aggregators is None\n",
    "        if new_agg:\n",
    "            aggregators = []\n",
    "        for layer in range(len(num_samples)):\n",
    "            if new_agg:\n",
    "                dim_mult = 2 if concat and (layer != 0) else 1\n",
    "                # aggregator at current layer\n",
    "                if layer == len(num_samples) - 1:\n",
    "                    aggregator = self.aggregator_cls(dim_mult*dims[layer], dims[layer+1], act=lambda x : x,\n",
    "                            dropout=self.placeholders['dropout'], \n",
    "                            name=name, concat=concat, model_size=model_size)\n",
    "                else:\n",
    "                    aggregator = self.aggregator_cls(dim_mult*dims[layer], dims[layer+1],\n",
    "                            dropout=self.placeholders['dropout'], \n",
    "                            name=name, concat=concat, model_size=model_size)\n",
    "                aggregators.append(aggregator)\n",
    "            else:\n",
    "                aggregator = aggregators[layer]\n",
    "            # hidden representation at current layer for all support nodes that are various hops away\n",
    "            next_hidden = []\n",
    "            # as layer increases, the number of support nodes needed decreases\n",
    "            for hop in range(len(num_samples) - layer):\n",
    "                dim_mult = 2 if concat and (layer != 0) else 1\n",
    "                neigh_dims = [batch_size * support_sizes[hop], \n",
    "                              num_samples[len(num_samples) - hop - 1], \n",
    "                              dim_mult*dims[layer]]\n",
    "                h = aggregator((hidden[hop],\n",
    "                                tf.reshape(hidden[hop + 1], neigh_dims)))\n",
    "                next_hidden.append(h)\n",
    "            hidden = next_hidden\n",
    "        return hidden[0], aggregators\n",
    "\n",
    "    def _build(self):\n",
    "        \n",
    "#         labels = tf.reshape(\n",
    "#                 tf.cast(self.placeholders['batch2'], dtype=tf.int64),\n",
    "#                 [self.batch_size, 1])\n",
    "        \n",
    "#         self.neg_samples, _, _ = (tf.nn.fixed_unigram_candidate_sampler(\n",
    "#             true_classes=labels,\n",
    "#             num_true=1,\n",
    "#             num_sampled=self.neg_sample_size,\n",
    "#             unique=False,\n",
    "#             range_max=len(self.degrees),\n",
    "#             distortion=0.75,\n",
    "#             unigrams=self.degrees.tolist()))\n",
    "    \n",
    "      \n",
    "        \n",
    "#         self.neg_samples = []\n",
    "#         elems = (self.inputs1, self.inputs2)\n",
    "        \n",
    "#         tf.map_fn(lambda x: x[0] * x[1], elems)\n",
    "            \n",
    "            \n",
    "#         for i,j in zip(labels, labels):\n",
    "#             tmp_sample = self.sample_table(1)[0]\n",
    "#             while(tmp_sample == i or tmp_sample == j):\n",
    "#                 tmp_sample = self.sample_table(1)[0]\n",
    "#             self.neg_samples.append(tmp_sample)\n",
    "           \n",
    "        # perform \"convolution\"\n",
    "        samples1, support_sizes1 = self.sample(self.inputs1, self.layer_infos)\n",
    "        samples2, support_sizes2 = self.sample(self.inputs2, self.layer_infos)\n",
    "        num_samples = [layer_info.num_samples for layer_info in self.layer_infos]\n",
    "        self.outputs1, self.aggregators = self.aggregate(samples1, [self.features], self.dims, num_samples,\n",
    "                support_sizes1, concat=self.concat, model_size=self.model_size)\n",
    "        self.outputs2, _ = self.aggregate(samples2, [self.features], self.dims, num_samples,\n",
    "                support_sizes2, aggregators=self.aggregators, concat=self.concat,\n",
    "                model_size=self.model_size)\n",
    "\n",
    "        neg_samples, neg_support_sizes = self.sample(self.neg_samples, self.layer_infos,\n",
    "            (self.batch_size*self.neg_sample_size))\n",
    "        self.neg_outputs, _ = self.aggregate(neg_samples, [self.features], self.dims, num_samples,\n",
    "                neg_support_sizes, batch_size=(self.batch_size*self.neg_sample_size), aggregators=self.aggregators,\n",
    "                concat=self.concat, model_size=self.model_size)\n",
    "\n",
    "        dim_mult = 2 if self.concat else 1\n",
    "        self.link_pred_layer = BipartiteEdgePredLayer(dim_mult*self.dims[-1],\n",
    "                dim_mult*self.dims[-1], self.placeholders, act=tf.nn.sigmoid, \n",
    "                bilinear_weights=False,\n",
    "                name='edge_predict')\n",
    "\n",
    "        self.outputs1 = tf.nn.l2_normalize(self.outputs1, 1)\n",
    "        self.outputs2 = tf.nn.l2_normalize(self.outputs2, 1)\n",
    "        self.neg_outputs = tf.nn.l2_normalize(self.neg_outputs, 1)\n",
    "\n",
    "    def build(self):\n",
    "        self._build()\n",
    "\n",
    "        # TF graph management\n",
    "        self._loss()\n",
    "        self._accuracy()\n",
    "        self.loss = self.loss / tf.cast(self.batch_size, tf.float32)\n",
    "        grads_and_vars = self.optimizer.compute_gradients(self.loss)\n",
    "        clipped_grads_and_vars = [(tf.clip_by_value(grad, -5.0, 5.0) if grad is not None else None, var) \n",
    "                for grad, var in grads_and_vars]\n",
    "        self.grad, _ = clipped_grads_and_vars[0]\n",
    "        self.opt_op = self.optimizer.apply_gradients(clipped_grads_and_vars)\n",
    "\n",
    "    def _loss(self):\n",
    "        for aggregator in self.aggregators:\n",
    "            for var in aggregator.vars.values():\n",
    "                self.loss += weight_decay * tf.nn.l2_loss(var)\n",
    "\n",
    "        self.loss += self.link_pred_layer.loss(self.outputs1, self.outputs2, self.neg_outputs) \n",
    "        tf.summary.scalar('loss', self.loss)\n",
    "\n",
    "    def _accuracy(self):\n",
    "        # shape: [batch_size]\n",
    "#         aff = self.link_pred_layer.affinity(self.outputs1, self.outputs2)\n",
    "        \n",
    "#         # shape : [batch_size x num_neg_samples]\n",
    "#         self.neg_aff = self.link_pred_layer.neg_cost(self.outputs1, self.neg_outputs)\n",
    "#         self.neg_aff = tf.reshape(self.neg_aff, [self.batch_size, self.neg_sample_size])\n",
    "        \n",
    "#         _aff = tf.expand_dims(aff, axis=1)\n",
    "#         self.aff_all = tf.concat(axis=1, values=[self.neg_aff, _aff])\n",
    "#         size = tf.shape(self.aff_all)[1]\n",
    "#         _, indices_of_ranks = tf.nn.top_k(self.aff_all, k=size)\n",
    "#         _, self.ranks = tf.nn.top_k(-indices_of_ranks, k=size)\n",
    "#         self.mrr = tf.reduce_mean(tf.div(1.0, tf.cast(self.ranks[:, -1] + 1, tf.float32)))\n",
    "#         tf.summary.scalar('mrr', self.mrr)\n",
    "        self.mrr = 0.0\n",
    "        tf.summary.scalar('mrr', self.mrr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading...\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "g = Graph()\n",
    "print(\"Reading...\")\n",
    "\n",
    "\n",
    "\n",
    "a_input = './../Tweet_edgelist.txt'\n",
    "\n",
    "a_weighted = False\n",
    "directed = False\n",
    "g.read_edgelist(filename=a_input, weighted=a_weighted,\n",
    "                        directed=directed)\n",
    "\n",
    "\n",
    "\n",
    "feature_file = './../Tweet_featurelist.txt'\n",
    "g.read_node_features(feature_file)\n",
    "\n",
    "dropout = 0.5\n",
    "weight_decay = 5e-4\n",
    "hidden = 16\n",
    "epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adj.shape :  (5099, 100)\n",
      "deg.shape :  (5098,)\n"
     ]
    }
   ],
   "source": [
    "context_pairs = list(g.G.edges())\n",
    "\n",
    "def construct_placeholders():\n",
    "    # Define placeholders\n",
    "    placeholders = {\n",
    "        'batch1' : tf.placeholder(tf.int32, shape=(None), name='batch1'),\n",
    "        'batch2' : tf.placeholder(tf.int32, shape=(None), name='batch2'),\n",
    "        # negative samples for all nodes in the batch\n",
    "        'neg_samples': tf.placeholder(tf.int32, shape=(None), name='neg_samples'),\n",
    "        'neg_sample_size': tf.placeholder(tf.int32, name='neg_sample_size'),\n",
    "        'dropout': tf.placeholder_with_default(0., shape=(), name='dropout'),\n",
    "        'batch_size' : tf.placeholder(tf.int32, name='batch_size'),\n",
    "    }\n",
    "    return placeholders\n",
    "\n",
    "placeholders = construct_placeholders()\n",
    "max_degree = 100\n",
    "adj = len(g.look_up_dict)*np.ones((len(g.look_up_dict)+1, max_degree))\n",
    "deg = np.zeros((len(g.look_up_dict),))\n",
    "print(\"adj.shape : \",adj.shape)\n",
    "print(\"deg.shape : \",deg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nodeid in g.G.nodes():\n",
    "    neighbors = np.array([g.look_up_dict[neighbor] for neighbor in g.G.neighbors(nodeid)])\n",
    "    deg[g.look_up_dict[nodeid]] = len(neighbors)\n",
    "    if len(neighbors) == 0:\n",
    "        continue\n",
    "    if len(neighbors) > max_degree:\n",
    "        neighbors = np.random.choice(neighbors, max_degree, replace=False)\n",
    "    elif len(neighbors) < max_degree:\n",
    "        neighbors = np.random.choice(neighbors, max_degree, replace=True)\n",
    "        \n",
    "    adj[g.look_up_dict[nodeid], :] = neighbors    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_info_ph = tf.placeholder(tf.int32, shape=adj.shape)\n",
    "adj_info = tf.Variable(adj_info_ph, trainable=False, name=\"adj_info\")\n",
    "\n",
    "SAGEInfo = namedtuple(\"SAGEInfo\",\n",
    "    ['layer_name', # name of the layer (to get feature embedding etc.)\n",
    "     'neigh_sampler', # callable neigh_sampler constructor\n",
    "     'num_samples',\n",
    "     'output_dim' # the output (i.e., hidden) dimension\n",
    "    ])\n",
    "\n",
    "samples_1 = 25\n",
    "samples_2 = 10\n",
    "\n",
    "#samples_1 = 10\n",
    "#samples_2 = 5\n",
    "\n",
    "\n",
    "dim_1 = 128\n",
    "dim_2 = 128\n",
    "\n",
    "sampler = UniformNeighborSampler(adj_info)\n",
    "layer_infos = [SAGEInfo(\"node\", sampler, samples_1, dim_1),\n",
    "            SAGEInfo(\"node\", sampler, samples_2, dim_2)]\n",
    "\n",
    "#features = None\n",
    "features = list(nx.get_node_attributes(g.G,'feature').values())\n",
    "features = np.asarray(features)\n",
    "features = np.vstack([features, np.zeros((features.shape[1],))])\n",
    "\n",
    "model_size = \"small\"\n",
    "identity_dim = adj_info.get_shape().as_list()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity_dim = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-268b04e0b3a9>:730: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "self.neg_sample_size  Tensor(\"neg_sample_size:0\", dtype=int32)\n",
      "self.batch_size  Tensor(\"batch_size:0\", dtype=int32)\n",
      "loss_fn  xent\n",
      "orginal neg_samples shape  (?, 128)\n",
      "reshape1 neg_samples shape  (?, ?, 128)\n",
      "reshape2 neg_samples shape  (?, 128, ?)\n",
      "orginal inputs1 shape  (?, 128)\n",
      "temp_neg_aff shape  (?, 1, ?)\n",
      "neg_samples shape  (?, 128, ?)\n",
      "inputs1 shape  (?, 1, 128)\n",
      "neg_aff shape  (?, ?)\n",
      "WARNING:tensorflow:From C:\\Users\\VULCAN\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model = SampleAndAggregate(placeholders, \n",
    "                             features,\n",
    "                             adj_info,\n",
    "                             deg,\n",
    "                             layer_infos=layer_infos, \n",
    "                             aggregator_type=\"gcn\",\n",
    "                             model_size=model_size,\n",
    "                             identity_dim = identity_dim,\n",
    "                             concat=False,\n",
    "                             logging=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling unigram table\n",
      "Finish filling unigram table\n"
     ]
    }
   ],
   "source": [
    "table = UnigramTable(deg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5099, 89)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto(log_device_placement=False)\n",
    "config.gpu_options.allow_growth = True\n",
    "config.allow_soft_placement = True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "sess.run(tf.global_variables_initializer(),\n",
    "         feed_dict={adj_info_ph: adj})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001\n",
      "Iter: 0000 pairs: 0.00 train_loss= 5.52921 time= 1.48445\n",
      "Iter: 0050 pairs: 9.01 train_loss= 5.46609 time= 0.04968\n",
      "Iter: 0100 pairs: 18.01 train_loss= 5.40103 time= 0.03554\n",
      "Iter: 0150 pairs: 27.02 train_loss= 5.29486 time= 0.03072\n",
      "Iter: 0200 pairs: 36.03 train_loss= 5.18134 time= 0.02825\n",
      "Iter: 0250 pairs: 45.03 train_loss= 5.09701 time= 0.02679\n",
      "Iter: 0300 pairs: 54.04 train_loss= 5.00796 time= 0.02583\n",
      "Iter: 0350 pairs: 63.05 train_loss= 4.84629 time= 0.02516\n",
      "Iter: 0400 pairs: 72.05 train_loss= 4.79254 time= 0.02462\n",
      "Iter: 0450 pairs: 81.06 train_loss= 4.68321 time= 0.02423\n",
      "Iter: 0500 pairs: 90.07 train_loss= 4.61659 time= 0.02393\n",
      "Iter: 0550 pairs: 99.07 train_loss= 4.54174 time= 0.02368\n",
      "Epoch: 0002\n",
      "Iter: 0000 pairs: 0.00 train_loss= 4.56243 time= 0.02363\n",
      "Iter: 0050 pairs: 9.01 train_loss= 4.47617 time= 0.02341\n",
      "Iter: 0100 pairs: 18.01 train_loss= 4.40499 time= 0.02324\n",
      "Iter: 0150 pairs: 27.02 train_loss= 4.35014 time= 0.02308\n",
      "Iter: 0200 pairs: 36.03 train_loss= 4.30525 time= 0.02294\n",
      "Iter: 0250 pairs: 45.03 train_loss= 4.25916 time= 0.02283\n",
      "Iter: 0300 pairs: 54.04 train_loss= 4.23007 time= 0.02273\n",
      "Iter: 0350 pairs: 63.05 train_loss= 4.20990 time= 0.02263\n",
      "Iter: 0400 pairs: 72.05 train_loss= 4.18489 time= 0.02254\n",
      "Iter: 0450 pairs: 81.06 train_loss= 4.16845 time= 0.02247\n",
      "Iter: 0500 pairs: 90.07 train_loss= 4.14213 time= 0.02240\n",
      "Iter: 0550 pairs: 99.07 train_loss= 4.12412 time= 0.02233\n",
      "Epoch: 0003\n",
      "Iter: 0000 pairs: 0.00 train_loss= 4.10655 time= 0.02231\n",
      "Iter: 0050 pairs: 9.01 train_loss= 4.13759 time= 0.02225\n",
      "Iter: 0100 pairs: 18.01 train_loss= 4.07387 time= 0.02219\n",
      "Iter: 0150 pairs: 27.02 train_loss= 4.11549 time= 0.02215\n",
      "Iter: 0200 pairs: 36.03 train_loss= 4.11309 time= 0.02210\n",
      "Iter: 0250 pairs: 45.03 train_loss= 4.07570 time= 0.02205\n",
      "Iter: 0300 pairs: 54.04 train_loss= 4.09743 time= 0.02201\n",
      "Iter: 0350 pairs: 63.05 train_loss= 4.07496 time= 0.02198\n",
      "Iter: 0400 pairs: 72.05 train_loss= 4.07242 time= 0.02194\n",
      "Iter: 0450 pairs: 81.06 train_loss= 4.07720 time= 0.02191\n",
      "Iter: 0500 pairs: 90.07 train_loss= 4.04587 time= 0.02189\n",
      "Iter: 0550 pairs: 99.07 train_loss= 4.06952 time= 0.02186\n",
      "Epoch: 0004\n",
      "Iter: 0000 pairs: 0.00 train_loss= 4.04484 time= 0.02185\n",
      "Iter: 0050 pairs: 9.01 train_loss= 4.04258 time= 0.02183\n",
      "Iter: 0100 pairs: 18.01 train_loss= 4.06094 time= 0.02181\n",
      "Iter: 0150 pairs: 27.02 train_loss= 4.04365 time= 0.02179\n",
      "Iter: 0200 pairs: 36.03 train_loss= 4.02899 time= 0.02177\n",
      "Iter: 0250 pairs: 45.03 train_loss= 4.05442 time= 0.02174\n",
      "Iter: 0300 pairs: 54.04 train_loss= 4.05864 time= 0.02172\n",
      "Iter: 0350 pairs: 63.05 train_loss= 4.07886 time= 0.02170\n",
      "Iter: 0400 pairs: 72.05 train_loss= 4.02550 time= 0.02168\n",
      "Iter: 0450 pairs: 81.06 train_loss= 4.05334 time= 0.02166\n",
      "Iter: 0500 pairs: 90.07 train_loss= 4.08761 time= 0.02165\n",
      "Iter: 0550 pairs: 99.07 train_loss= 4.05252 time= 0.02163\n",
      "Epoch: 0005\n",
      "Iter: 0000 pairs: 0.00 train_loss= 4.06283 time= 0.02163\n",
      "Iter: 0050 pairs: 9.01 train_loss= 4.07430 time= 0.02161\n",
      "Iter: 0100 pairs: 18.01 train_loss= 4.03923 time= 0.02160\n",
      "Iter: 0150 pairs: 27.02 train_loss= 4.08032 time= 0.02158\n",
      "Iter: 0200 pairs: 36.03 train_loss= 4.06623 time= 0.02157\n",
      "Iter: 0250 pairs: 45.03 train_loss= 4.06954 time= 0.02156\n",
      "Iter: 0300 pairs: 54.04 train_loss= 4.07099 time= 0.02154\n",
      "Iter: 0350 pairs: 63.05 train_loss= 4.05690 time= 0.02153\n",
      "Iter: 0400 pairs: 72.05 train_loss= 4.05562 time= 0.02151\n",
      "Iter: 0450 pairs: 81.06 train_loss= 4.05870 time= 0.02150\n",
      "Iter: 0500 pairs: 90.07 train_loss= 4.03254 time= 0.02149\n",
      "Iter: 0550 pairs: 99.07 train_loss= 4.06406 time= 0.02148\n",
      "Epoch: 0006\n",
      "Iter: 0000 pairs: 0.00 train_loss= 4.04628 time= 0.02147\n",
      "Iter: 0050 pairs: 9.01 train_loss= 4.03272 time= 0.02146\n",
      "Iter: 0100 pairs: 18.01 train_loss= 4.05831 time= 0.02145\n",
      "Iter: 0150 pairs: 27.02 train_loss= 4.05972 time= 0.02144\n",
      "Iter: 0200 pairs: 36.03 train_loss= 4.05373 time= 0.02143\n",
      "Iter: 0250 pairs: 45.03 train_loss= 4.04507 time= 0.02142\n",
      "Iter: 0300 pairs: 54.04 train_loss= 4.06179 time= 0.02141\n",
      "Iter: 0350 pairs: 63.05 train_loss= 4.06938 time= 0.02140\n",
      "Iter: 0400 pairs: 72.05 train_loss= 4.05189 time= 0.02140\n",
      "Iter: 0450 pairs: 81.06 train_loss= 4.05311 time= 0.02138\n",
      "Iter: 0500 pairs: 90.07 train_loss= 4.05769 time= 0.02138\n",
      "Iter: 0550 pairs: 99.07 train_loss= 4.07405 time= 0.02137\n",
      "Epoch: 0007\n",
      "Iter: 0000 pairs: 0.00 train_loss= 4.05196 time= 0.02137\n",
      "Iter: 0050 pairs: 9.01 train_loss= 4.02675 time= 0.02136\n",
      "Iter: 0100 pairs: 18.01 train_loss= 4.04166 time= 0.02135\n",
      "Iter: 0150 pairs: 27.02 train_loss= 4.03750 time= 0.02135\n",
      "Iter: 0200 pairs: 36.03 train_loss= 4.07418 time= 0.02134\n",
      "Iter: 0250 pairs: 45.03 train_loss= 4.06033 time= 0.02134\n",
      "Iter: 0300 pairs: 54.04 train_loss= 4.02543 time= 0.02133\n",
      "Iter: 0350 pairs: 63.05 train_loss= 4.05886 time= 0.02132\n",
      "Iter: 0400 pairs: 72.05 train_loss= 4.07302 time= 0.02131\n",
      "Iter: 0450 pairs: 81.06 train_loss= 4.03092 time= 0.02131\n",
      "Iter: 0500 pairs: 90.07 train_loss= 4.03844 time= 0.02130\n",
      "Iter: 0550 pairs: 99.07 train_loss= 4.05916 time= 0.02130\n",
      "Epoch: 0008\n",
      "Iter: 0000 pairs: 0.00 train_loss= 4.06560 time= 0.02129\n",
      "Iter: 0050 pairs: 9.01 train_loss= 4.05259 time= 0.02129\n",
      "Iter: 0100 pairs: 18.01 train_loss= 4.05234 time= 0.02129\n",
      "Iter: 0150 pairs: 27.02 train_loss= 4.03722 time= 0.02128\n",
      "Iter: 0200 pairs: 36.03 train_loss= 4.03348 time= 0.02128\n",
      "Iter: 0250 pairs: 45.03 train_loss= 4.05974 time= 0.02127\n",
      "Iter: 0300 pairs: 54.04 train_loss= 4.03084 time= 0.02127\n",
      "Iter: 0350 pairs: 63.05 train_loss= 4.03488 time= 0.02126\n",
      "Iter: 0400 pairs: 72.05 train_loss= 4.04897 time= 0.02126\n",
      "Iter: 0450 pairs: 81.06 train_loss= 4.06724 time= 0.02126\n",
      "Iter: 0500 pairs: 90.07 train_loss= 4.03941 time= 0.02125\n",
      "Iter: 0550 pairs: 99.07 train_loss= 4.04043 time= 0.02125\n",
      "Epoch: 0009\n",
      "Iter: 0000 pairs: 0.00 train_loss= 4.03890 time= 0.02125\n",
      "Iter: 0050 pairs: 9.01 train_loss= 4.05693 time= 0.02124\n",
      "Iter: 0100 pairs: 18.01 train_loss= 4.06689 time= 0.02123\n",
      "Iter: 0150 pairs: 27.02 train_loss= 4.05595 time= 0.02123\n",
      "Iter: 0200 pairs: 36.03 train_loss= 4.02947 time= 0.02123\n",
      "Iter: 0250 pairs: 45.03 train_loss= 4.01281 time= 0.02123\n",
      "Iter: 0300 pairs: 54.04 train_loss= 4.03554 time= 0.02123\n",
      "Iter: 0350 pairs: 63.05 train_loss= 4.05790 time= 0.02122\n",
      "Iter: 0400 pairs: 72.05 train_loss= 4.03525 time= 0.02122\n",
      "Iter: 0450 pairs: 81.06 train_loss= 4.07117 time= 0.02122\n",
      "Iter: 0500 pairs: 90.07 train_loss= 4.07986 time= 0.02121\n",
      "Iter: 0550 pairs: 99.07 train_loss= 4.01457 time= 0.02120\n",
      "Epoch: 0010\n",
      "Iter: 0000 pairs: 0.00 train_loss= 4.06773 time= 0.02120\n",
      "Iter: 0050 pairs: 9.01 train_loss= 4.03993 time= 0.02120\n",
      "Iter: 0100 pairs: 18.01 train_loss= 4.02648 time= 0.02119\n",
      "Iter: 0150 pairs: 27.02 train_loss= 4.05529 time= 0.02119\n",
      "Iter: 0200 pairs: 36.03 train_loss= 4.05788 time= 0.02119\n",
      "Iter: 0250 pairs: 45.03 train_loss= 4.03284 time= 0.02118\n",
      "Iter: 0300 pairs: 54.04 train_loss= 4.02670 time= 0.02118\n",
      "Iter: 0350 pairs: 63.05 train_loss= 4.09896 time= 0.02118\n",
      "Iter: 0400 pairs: 72.05 train_loss= 4.02048 time= 0.02117\n",
      "Iter: 0450 pairs: 81.06 train_loss= 4.04016 time= 0.02117\n",
      "Iter: 0500 pairs: 90.07 train_loss= 4.07549 time= 0.02117\n",
      "Iter: 0550 pairs: 99.07 train_loss= 4.05806 time= 0.02117\n",
      "Epoch: 0011\n",
      "Iter: 0000 pairs: 0.00 train_loss= 4.03732 time= 0.02117\n",
      "Iter: 0050 pairs: 9.01 train_loss= 4.03532 time= 0.02116\n",
      "Iter: 0100 pairs: 18.01 train_loss= 4.04251 time= 0.02116\n",
      "Iter: 0150 pairs: 27.02 train_loss= 4.07164 time= 0.02115\n",
      "Iter: 0200 pairs: 36.03 train_loss= 4.04628 time= 0.02115\n",
      "Iter: 0250 pairs: 45.03 train_loss= 4.06827 time= 0.02114\n",
      "Iter: 0300 pairs: 54.04 train_loss= 4.07782 time= 0.02114\n",
      "Iter: 0350 pairs: 63.05 train_loss= 4.06501 time= 0.02114\n",
      "Iter: 0400 pairs: 72.05 train_loss= 4.04367 time= 0.02114\n",
      "Iter: 0450 pairs: 81.06 train_loss= 4.04006 time= 0.02114\n",
      "Iter: 0500 pairs: 90.07 train_loss= 4.01276 time= 0.02113\n",
      "Iter: 0550 pairs: 99.07 train_loss= 4.05813 time= 0.02113\n",
      "Epoch: 0012\n",
      "Iter: 0000 pairs: 0.00 train_loss= 4.04941 time= 0.02113\n",
      "Iter: 0050 pairs: 9.01 train_loss= 4.04675 time= 0.02113\n",
      "Iter: 0100 pairs: 18.01 train_loss= 4.03127 time= 0.02113\n",
      "Iter: 0150 pairs: 27.02 train_loss= 4.05745 time= 0.02112\n",
      "Iter: 0200 pairs: 36.03 train_loss= 4.02403 time= 0.02112\n",
      "Iter: 0250 pairs: 45.03 train_loss= 4.07503 time= 0.02112\n",
      "Iter: 0300 pairs: 54.04 train_loss= 4.04573 time= 0.02112\n",
      "Iter: 0350 pairs: 63.05 train_loss= 4.04269 time= 0.02112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0400 pairs: 72.05 train_loss= 4.03508 time= 0.02112\n",
      "Iter: 0450 pairs: 81.06 train_loss= 4.03134 time= 0.02111\n",
      "Iter: 0500 pairs: 90.07 train_loss= 4.01745 time= 0.02111\n",
      "Iter: 0550 pairs: 99.07 train_loss= 4.05952 time= 0.02111\n",
      "Epoch: 0013\n",
      "Iter: 0000 pairs: 0.00 train_loss= 4.02696 time= 0.02111\n",
      "Iter: 0050 pairs: 9.01 train_loss= 4.05854 time= 0.02111\n",
      "Iter: 0100 pairs: 18.01 train_loss= 4.06707 time= 0.02111\n",
      "Iter: 0150 pairs: 27.02 train_loss= 4.01103 time= 0.02110\n",
      "Iter: 0200 pairs: 36.03 train_loss= 4.08762 time= 0.02110\n",
      "Iter: 0250 pairs: 45.03 train_loss= 4.04382 time= 0.02110\n",
      "Iter: 0300 pairs: 54.04 train_loss= 4.03828 time= 0.02110\n",
      "Iter: 0350 pairs: 63.05 train_loss= 4.03300 time= 0.02110\n",
      "Iter: 0400 pairs: 72.05 train_loss= 4.05456 time= 0.02110\n",
      "Iter: 0450 pairs: 81.06 train_loss= 4.03315 time= 0.02110\n",
      "Iter: 0500 pairs: 90.07 train_loss= 4.03373 time= 0.02110\n",
      "Iter: 0550 pairs: 99.07 train_loss= 4.04827 time= 0.02109\n",
      "Epoch: 0014\n",
      "Iter: 0000 pairs: 0.00 train_loss= 4.03530 time= 0.02109\n",
      "Iter: 0050 pairs: 9.01 train_loss= 4.05124 time= 0.02109\n",
      "Iter: 0100 pairs: 18.01 train_loss= 4.03308 time= 0.02109\n",
      "Iter: 0150 pairs: 27.02 train_loss= 4.06098 time= 0.02109\n",
      "Iter: 0200 pairs: 36.03 train_loss= 4.03818 time= 0.02109\n",
      "Iter: 0250 pairs: 45.03 train_loss= 4.05297 time= 0.02109\n",
      "Iter: 0300 pairs: 54.04 train_loss= 4.04077 time= 0.02109\n",
      "Iter: 0350 pairs: 63.05 train_loss= 4.04746 time= 0.02109\n",
      "Iter: 0400 pairs: 72.05 train_loss= 4.03281 time= 0.02108\n",
      "Iter: 0450 pairs: 81.06 train_loss= 4.04778 time= 0.02108\n",
      "Iter: 0500 pairs: 90.07 train_loss= 4.04516 time= 0.02108\n",
      "Iter: 0550 pairs: 99.07 train_loss= 4.02836 time= 0.02108\n",
      "Epoch: 0015\n",
      "Iter: 0000 pairs: 0.00 train_loss= 4.02873 time= 0.02108\n",
      "Iter: 0050 pairs: 9.01 train_loss= 4.05017 time= 0.02108\n",
      "Iter: 0100 pairs: 18.01 train_loss= 4.03425 time= 0.02108\n",
      "Iter: 0150 pairs: 27.02 train_loss= 4.04916 time= 0.02108\n",
      "Iter: 0200 pairs: 36.03 train_loss= 4.05069 time= 0.02108\n",
      "Iter: 0250 pairs: 45.03 train_loss= 4.00526 time= 0.02108\n",
      "Iter: 0300 pairs: 54.04 train_loss= 4.07170 time= 0.02108\n",
      "Iter: 0350 pairs: 63.05 train_loss= 4.03624 time= 0.02107\n",
      "Iter: 0400 pairs: 72.05 train_loss= 4.03535 time= 0.02107\n",
      "Iter: 0450 pairs: 81.06 train_loss= 4.05558 time= 0.02107\n",
      "Iter: 0500 pairs: 90.07 train_loss= 4.00559 time= 0.02106\n",
      "Iter: 0550 pairs: 99.07 train_loss= 4.04103 time= 0.02106\n",
      "Epoch: 0016\n",
      "Iter: 0000 pairs: 0.00 train_loss= 4.02221 time= 0.02106\n",
      "Iter: 0050 pairs: 9.01 train_loss= 4.03690 time= 0.02106\n",
      "Iter: 0100 pairs: 18.01 train_loss= 4.03585 time= 0.02106\n",
      "Iter: 0150 pairs: 27.02 train_loss= 4.03117 time= 0.02106\n",
      "Iter: 0200 pairs: 36.03 train_loss= 4.00683 time= 0.02106\n",
      "Iter: 0250 pairs: 45.03 train_loss= 4.04664 time= 0.02106\n",
      "Iter: 0300 pairs: 54.04 train_loss= 4.02343 time= 0.02105\n",
      "Iter: 0350 pairs: 63.05 train_loss= 4.03265 time= 0.02105\n",
      "Iter: 0400 pairs: 72.05 train_loss= 4.01417 time= 0.02105\n",
      "Iter: 0450 pairs: 81.06 train_loss= 4.01732 time= 0.02105\n",
      "Iter: 0500 pairs: 90.07 train_loss= 4.06819 time= 0.02105\n",
      "Iter: 0550 pairs: 99.07 train_loss= 4.02699 time= 0.02105\n",
      "Epoch: 0017\n",
      "Iter: 0000 pairs: 0.00 train_loss= 4.05249 time= 0.02105\n",
      "Iter: 0050 pairs: 9.01 train_loss= 4.02152 time= 0.02105\n",
      "Iter: 0100 pairs: 18.01 train_loss= 4.06478 time= 0.02104\n",
      "Iter: 0150 pairs: 27.02 train_loss= 4.05643 time= 0.02104\n",
      "Iter: 0200 pairs: 36.03 train_loss= 4.02271 time= 0.02104\n",
      "Iter: 0250 pairs: 45.03 train_loss= 4.01968 time= 0.02104\n",
      "Iter: 0300 pairs: 54.04 train_loss= 4.03647 time= 0.02104\n",
      "Iter: 0350 pairs: 63.05 train_loss= 4.04377 time= 0.02104\n",
      "Iter: 0400 pairs: 72.05 train_loss= 4.02247 time= 0.02104\n",
      "Iter: 0450 pairs: 81.06 train_loss= 4.04385 time= 0.02104\n",
      "Iter: 0500 pairs: 90.07 train_loss= 4.03282 time= 0.02104\n",
      "Iter: 0550 pairs: 99.07 train_loss= 4.07322 time= 0.02104\n",
      "Epoch: 0018\n",
      "Iter: 0000 pairs: 0.00 train_loss= 4.07256 time= 0.02104\n",
      "Iter: 0050 pairs: 9.01 train_loss= 4.08567 time= 0.02104\n",
      "Iter: 0100 pairs: 18.01 train_loss= 4.04639 time= 0.02104\n",
      "Iter: 0150 pairs: 27.02 train_loss= 4.01761 time= 0.02104\n",
      "Iter: 0200 pairs: 36.03 train_loss= 4.03771 time= 0.02104\n",
      "Iter: 0250 pairs: 45.03 train_loss= 4.04674 time= 0.02103\n",
      "Iter: 0300 pairs: 54.04 train_loss= 4.04759 time= 0.02103\n",
      "Iter: 0350 pairs: 63.05 train_loss= 4.02666 time= 0.02103\n",
      "Iter: 0400 pairs: 72.05 train_loss= 4.02442 time= 0.02103\n",
      "Iter: 0450 pairs: 81.06 train_loss= 4.06169 time= 0.02103\n",
      "Iter: 0500 pairs: 90.07 train_loss= 4.04480 time= 0.02103\n",
      "Iter: 0550 pairs: 99.07 train_loss= 4.03892 time= 0.02103\n",
      "Epoch: 0019\n",
      "Iter: 0000 pairs: 0.00 train_loss= 4.05849 time= 0.02103\n",
      "Iter: 0050 pairs: 9.01 train_loss= 4.07712 time= 0.02103\n",
      "Iter: 0100 pairs: 18.01 train_loss= 4.06677 time= 0.02103\n",
      "Iter: 0150 pairs: 27.02 train_loss= 4.02478 time= 0.02103\n",
      "Iter: 0200 pairs: 36.03 train_loss= 4.06357 time= 0.02103\n",
      "Iter: 0250 pairs: 45.03 train_loss= 4.04452 time= 0.02103\n",
      "Iter: 0300 pairs: 54.04 train_loss= 4.04545 time= 0.02103\n",
      "Iter: 0350 pairs: 63.05 train_loss= 4.05143 time= 0.02102\n",
      "Iter: 0400 pairs: 72.05 train_loss= 4.03067 time= 0.02102\n",
      "Iter: 0450 pairs: 81.06 train_loss= 4.01306 time= 0.02102\n",
      "Iter: 0500 pairs: 90.07 train_loss= 4.05093 time= 0.02102\n",
      "Iter: 0550 pairs: 99.07 train_loss= 4.05810 time= 0.02102\n",
      "Epoch: 0020\n",
      "Iter: 0000 pairs: 0.00 train_loss= 4.03041 time= 0.02102\n",
      "Iter: 0050 pairs: 9.01 train_loss= 4.03432 time= 0.02102\n",
      "Iter: 0100 pairs: 18.01 train_loss= 4.03278 time= 0.02102\n",
      "Iter: 0150 pairs: 27.02 train_loss= 4.03820 time= 0.02102\n",
      "Iter: 0200 pairs: 36.03 train_loss= 4.06329 time= 0.02102\n",
      "Iter: 0250 pairs: 45.03 train_loss= 4.04496 time= 0.02102\n",
      "Iter: 0300 pairs: 54.04 train_loss= 4.05950 time= 0.02102\n",
      "Iter: 0350 pairs: 63.05 train_loss= 4.02389 time= 0.02102\n",
      "Iter: 0400 pairs: 72.05 train_loss= 4.01753 time= 0.02102\n",
      "Iter: 0450 pairs: 81.06 train_loss= 4.02830 time= 0.02101\n",
      "Iter: 0500 pairs: 90.07 train_loss= 4.05809 time= 0.02101\n",
      "Iter: 0550 pairs: 99.07 train_loss= 4.05501 time= 0.02101\n"
     ]
    }
   ],
   "source": [
    "total_steps = 0\n",
    "epochs = 20\n",
    "\n",
    "batch_size= 200\n",
    "avg_time = 0.0\n",
    "each_node_neg_samples = 5\n",
    "total_len = len(context_pairs)\n",
    "\n",
    "for epoch in range(epochs): \n",
    "    train_edges = edges = np.random.permutation(context_pairs)\n",
    "    batch_num = 0\n",
    "    iter = 0\n",
    "    print('Epoch: %04d' % (epoch + 1))\n",
    "\n",
    "    while not (batch_num * batch_size >= len(train_edges)):\n",
    "\n",
    "        start_idx = batch_num * batch_size\n",
    "        batch_num += 1\n",
    "        end_idx = min(start_idx + batch_size, len(train_edges))\n",
    "        batch_nodes = train_edges[start_idx : end_idx]\n",
    "\n",
    "        batch1 = []\n",
    "        batch2 = []\n",
    "        neg_samples = []\n",
    "        \n",
    "        for node1, node2 in batch_nodes:\n",
    "            \n",
    "            seq_node1 = g.look_up_dict[node1]\n",
    "            seq_node2 = g.look_up_dict[node2]\n",
    "            \n",
    "            batch1.append(seq_node1)\n",
    "            batch2.append(seq_node2)\n",
    "            \n",
    "            \n",
    "            for i in np.arange(each_node_neg_samples):\n",
    "                tmp_sample = table.sample(1)[0]\n",
    "                while tmp_sample == seq_node1 or tmp_sample == seq_node2:\n",
    "                    tmp_sample = table.sample(1)[0]\n",
    "                neg_samples.append(tmp_sample)\n",
    "\n",
    "        feed_dict = dict()\n",
    "        feed_dict.update({placeholders['batch_size'] : len(batch_nodes)})\n",
    "        feed_dict.update({placeholders['batch1']: batch1})\n",
    "        feed_dict.update({placeholders['batch2']: batch2})\n",
    "        feed_dict.update({placeholders['neg_samples']: neg_samples})\n",
    "        feed_dict.update({placeholders['neg_sample_size']: each_node_neg_samples})\n",
    "        \n",
    "        #print(feed_dict)\n",
    "        #print()\n",
    "        \n",
    "        feed_dict.update({placeholders['dropout']: dropout})\n",
    "\n",
    "        t = time.time()\n",
    "        outs = sess.run([model.opt_op, model.loss,\n",
    "                         model.outputs1, model.outputs2, model.neg_outputs], feed_dict=feed_dict)   \n",
    "\n",
    "        train_cost = outs[1]\n",
    "        #train_mrr = outs[2]\n",
    "\n",
    "        avg_time = (avg_time * total_steps + time.time() - t) / (total_steps + 1)\n",
    "        \n",
    "        if iter%50 == 0:\n",
    "            print(\"Iter:\", '%04d' % iter, \n",
    "                              \"pairs:\", \"{:.2f}\".format(iter*batch_size/total_len*100), \n",
    "                              \"train_loss=\", \"{:.5f}\".format(train_cost),\n",
    "                              \"time=\", \"{:.5f}\".format(avg_time))\n",
    "\n",
    "        iter += 1\n",
    "        total_steps += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(y_true, y_pred):\n",
    "    y_true = y_true.astype(np.int64)\n",
    "    assert y_pred.size == y_true.size\n",
    "    D = max(y_pred.max(), y_true.max()) + 1\n",
    "    w = np.zeros((D, D), dtype=np.int64)\n",
    "    for i in range(y_pred.size):\n",
    "        w[y_pred[i], y_true[i]] += 1\n",
    "    from sklearn.utils.linear_assignment_ import linear_assignment\n",
    "    ind = linear_assignment(w.max() - w)\n",
    "    return sum([w[i, j] for i, j in ind]) * 1.0 / y_pred.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_embeddings = []\n",
    "finished = False\n",
    "seen = set([])\n",
    "nodes = []\n",
    "iter_num = 0\n",
    "name = \"val\"\n",
    "size = batch_size\n",
    "node_list = list(g.G.nodes())\n",
    "re_order_vectors = np.zeros((len(node_list),dim_1))\n",
    "\n",
    "while not finished:\n",
    "    \n",
    "    val_nodes = node_list[iter_num*size:min((iter_num+1)*size, len(node_list))]\n",
    "    val_edges = [(n,n) for n in val_nodes]\n",
    "\n",
    "    batch1 = []\n",
    "    batch2 = []\n",
    "    neg_samples = []\n",
    "    \n",
    "    for node1, node2 in val_edges:\n",
    "        seq_node1 = g.look_up_dict[node1]\n",
    "        seq_node2 = g.look_up_dict[node2]\n",
    "\n",
    "        batch1.append(seq_node1)\n",
    "        batch2.append(seq_node2)\n",
    "        tmp_sample = table.sample(1)[0]\n",
    "        while tmp_sample == seq_node1 or tmp_sample == seq_node2:\n",
    "            tmp_sample = table.sample(1)[0]\n",
    "        neg_samples.append(tmp_sample)\n",
    "     \n",
    "    feed_dict = dict()\n",
    "    feed_dict.update({placeholders['batch_size'] : len(val_edges)})\n",
    "    feed_dict.update({placeholders['batch1']: batch1})\n",
    "    feed_dict.update({placeholders['batch2']: batch2})\n",
    "    \n",
    "    \n",
    "    feed_dict.update({placeholders['neg_samples']: neg_samples})\n",
    "    feed_dict.update({placeholders['neg_sample_size']: 1})\n",
    "        \n",
    "    feed_dict.update({placeholders['dropout']: dropout})\n",
    "\n",
    "    feed_dict_val, finished, edges = feed_dict, (iter_num+1)*size >= len(node_list), val_nodes\n",
    "\n",
    "\n",
    "    iter_num += 1\n",
    "    outs_val = sess.run([model.loss, model.outputs1], \n",
    "                        feed_dict=feed_dict_val)\n",
    "    \n",
    "\n",
    "    \n",
    "    for i, edge in enumerate(edges):\n",
    "        #print(type(edge))  \n",
    "        if not edge in seen:\n",
    "            val_embeddings.append(outs_val[-1][i,:])\n",
    "            nodes.append(edge)\n",
    "            seen.add(edge)\n",
    "            re_order_vectors[int(edge)] = outs_val[-1][i,:]\n",
    "            \n",
    "val_embeddings = np.vstack(val_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"./../Tweet_dictionary.txt\"\n",
    "word_id = {}\n",
    "with open(filename,'r')as datafile:\n",
    "    sents = datafile.readlines()\n",
    "    for data in sents:\n",
    "        word_id[data.split(\" \")[0]] = int(data.split(\" \")[1].strip())\n",
    "datafile.close()\n",
    "\n",
    "import numpy as np\n",
    "total_doc_num = 2472\n",
    "doc_emb = np.zeros((total_doc_num,dim_1))\n",
    "doc_label = np.zeros((total_doc_num,),dtype=int)\n",
    "import re\n",
    "filename = \"./../data/Tweet\"\n",
    "doc_num = 0\n",
    "with open(filename,'r')as datafile:\n",
    "        sents = datafile.readlines()\n",
    "        punc = '\",:'\n",
    "        for data in sents:\n",
    "            \n",
    "            data = re.sub(r'[{}]+'.format(punc),'',data)\n",
    "            raw_text = data.split(' ')[1:-2]\n",
    "            \n",
    "            doc_label[doc_num] = int(data.split(' ')[-1].strip().replace(\"}\",\"\"))\n",
    "            for data_i in raw_text:\n",
    "                doc_emb[doc_num] += re_order_vectors[word_id[data_i]]\n",
    "            doc_emb[doc_num] /= len(raw_text)  \n",
    "            #doc_emb[doc_num]\n",
    "            doc_num += 1\n",
    "datafile.close()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=89,  max_iter=100).fit(doc_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2472,)\n",
      "(2472,)\n"
     ]
    }
   ],
   "source": [
    "print(kmeans.labels_.shape)\n",
    "print(doc_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import normalized_mutual_info_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8198505232518324\n",
      "0.5570388349514563\n"
     ]
    }
   ],
   "source": [
    "print(normalized_mutual_info_score(np.array(doc_label), np.array(kmeans.labels_)))\n",
    "print(acc(np.array(doc_label), np.array(kmeans.labels_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_nmi:  0.8150889401944876\n",
      "avg_acc:  0.5509911003236246\n"
     ]
    }
   ],
   "source": [
    "avg_nmi = []\n",
    "avg_acc = []\n",
    "for i in range(20):\n",
    "    kmeans = KMeans(n_clusters=89,  max_iter=100).fit(doc_emb)\n",
    "    avg_nmi.append(normalized_mutual_info_score(np.array(doc_label), np.array(kmeans.labels_)))\n",
    "    avg_acc.append(acc(np.array(doc_label), np.array(kmeans.labels_)))\n",
    "print(\"avg_nmi: \", np.mean(avg_nmi))\n",
    "print(\"avg_acc: \",np.mean(avg_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storefile = 'emb/Tweet_graphsage_emb.txt'\n",
    "# sf = open(storefile,'w')\n",
    "# for i in range(doc_emb.shape[0]):\n",
    "#     sf.write(str(i)+\" \"+\" \".join([str(ele) for ele in doc_emb[i]])+'\\n')\n",
    "# sf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
